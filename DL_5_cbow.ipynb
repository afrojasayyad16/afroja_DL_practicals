{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5540cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # Import Tokenizer from tensorflow.keras.preprocessing.text\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e596b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking random sentences as data\n",
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "dl_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae494900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 75\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "#Tokenizer was already imported from tensorflow.keras.preprocessing.text\n",
    "tokenizer = Tokenizer() #Calling Tokenizer directly \n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "#Calling text_to_word_sequence directly on tensorflow.keras.preprocessing.text\n",
    "wids = [[word2id[w] for w in tf.keras.preprocessing.text.text_to_word_sequence(doc)] for doc in dl_data] \n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d236309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating (context word, target/label word) pairs\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "449d4937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#model building\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: tf.reduce_mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(cbow.summary())\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99dac9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 430.84026861190796\n",
      "\n",
      "Epoch: 2 \tLoss: 430.6036448478699\n",
      "\n",
      "Epoch: 3 \tLoss: 428.8011283874512\n",
      "\n",
      "Epoch: 4 \tLoss: 427.0268521308899\n",
      "\n",
      "Epoch: 5 \tLoss: 425.5257782936096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "accf5b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0.002267</td>\n",
       "      <td>-0.007789</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.038082</td>\n",
       "      <td>-0.024031</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>-0.044413</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>-0.064489</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039946</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>-0.027129</td>\n",
       "      <td>0.023011</td>\n",
       "      <td>0.064948</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>-0.034359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>-0.018856</td>\n",
       "      <td>-0.030122</td>\n",
       "      <td>0.018339</td>\n",
       "      <td>0.039654</td>\n",
       "      <td>-0.038074</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.058217</td>\n",
       "      <td>-0.058789</td>\n",
       "      <td>-0.037672</td>\n",
       "      <td>0.031065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029049</td>\n",
       "      <td>-0.031998</td>\n",
       "      <td>0.010577</td>\n",
       "      <td>-0.063889</td>\n",
       "      <td>-0.025821</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>-0.012046</td>\n",
       "      <td>-0.022789</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>-0.004106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.007200</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>0.047119</td>\n",
       "      <td>-0.018791</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.031527</td>\n",
       "      <td>-0.042072</td>\n",
       "      <td>-0.012666</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.013458</td>\n",
       "      <td>0.030976</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>-0.021181</td>\n",
       "      <td>0.040395</td>\n",
       "      <td>-0.033758</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>-0.011201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.037502</td>\n",
       "      <td>-0.029883</td>\n",
       "      <td>0.040434</td>\n",
       "      <td>0.008709</td>\n",
       "      <td>0.048017</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.034919</td>\n",
       "      <td>-0.026568</td>\n",
       "      <td>0.032635</td>\n",
       "      <td>0.011609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002362</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>-0.020262</td>\n",
       "      <td>-0.046418</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>-0.007239</td>\n",
       "      <td>-0.012327</td>\n",
       "      <td>0.031023</td>\n",
       "      <td>-0.019619</td>\n",
       "      <td>0.006119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.008086</td>\n",
       "      <td>-0.013236</td>\n",
       "      <td>-0.001055</td>\n",
       "      <td>0.032365</td>\n",
       "      <td>0.044762</td>\n",
       "      <td>-0.029319</td>\n",
       "      <td>-0.009214</td>\n",
       "      <td>0.041468</td>\n",
       "      <td>-0.019296</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024535</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>0.036546</td>\n",
       "      <td>0.014987</td>\n",
       "      <td>-0.004683</td>\n",
       "      <td>0.011302</td>\n",
       "      <td>-0.019502</td>\n",
       "      <td>-0.031948</td>\n",
       "      <td>0.022799</td>\n",
       "      <td>0.037477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "deep      0.002267 -0.007789  0.017094  0.038082 -0.024031  0.063732   \n",
       "networks -0.018856 -0.030122  0.018339  0.039654 -0.038074  0.002258   \n",
       "neural    0.007200 -0.001533  0.047119 -0.018791  0.000139 -0.031527   \n",
       "and      -0.037502 -0.029883  0.040434  0.008709  0.048017  0.041638   \n",
       "as        0.008086 -0.013236 -0.001055  0.032365  0.044762 -0.029319   \n",
       "\n",
       "                6         7         8         9   ...        90        91  \\\n",
       "deep     -0.044413  0.001810  0.024153 -0.064489  ... -0.039946  0.005762   \n",
       "networks  0.058217 -0.058789 -0.037672  0.031065  ...  0.029049 -0.031998   \n",
       "neural   -0.042072 -0.012666  0.002303  0.020236  ...  0.008419  0.014360   \n",
       "and       0.034919 -0.026568  0.032635  0.011609  ... -0.002362  0.009720   \n",
       "as       -0.009214  0.041468 -0.019296 -0.013002  ... -0.024535 -0.005102   \n",
       "\n",
       "                92        93        94        95        96        97  \\\n",
       "deep     -0.027129  0.023011  0.064948  0.023818  0.014393  0.009237   \n",
       "networks  0.010577 -0.063889 -0.025821  0.026566 -0.012046 -0.022789   \n",
       "neural    0.013458  0.030976 -0.001089 -0.021181  0.040395 -0.033758   \n",
       "and      -0.020262 -0.046418  0.018378 -0.007239 -0.012327  0.031023   \n",
       "as        0.036546  0.014987 -0.004683  0.011302 -0.019502 -0.031948   \n",
       "\n",
       "                98        99  \n",
       "deep      0.001289 -0.034359  \n",
       "networks  0.037680 -0.004106  \n",
       "neural   -0.035054 -0.011201  \n",
       "and      -0.019619  0.006119  \n",
       "as        0.022799  0.037477  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cff7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
